{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.manifold import Isomap\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dfs = []\n",
    "    for filename in os.listdir('CSVs/'):\n",
    "        year_df = pd.read_csv('CSVs/'+filename)\n",
    "        year_df['winner'] = 1*(year_df['rank'] == 1)\n",
    "        dfs.append(year_df)\n",
    "    df = pd.concat(dfs)\n",
    "    df.index = range(0,df.shape[0])\n",
    "    return df\n",
    "\n",
    "def process_data(df):\n",
    "    \n",
    "    # defining country mapping\n",
    "    country_to_id_mapping = {}\n",
    "    id_to_country_mapping = {}\n",
    "    \n",
    "    id_count = 0\n",
    "    for code in df['code'].unique():\n",
    "        country_to_id_mapping[code] = id_count\n",
    "        id_to_country_mapping[id_count] = code\n",
    "        \n",
    "        id_count += 1\n",
    "        \n",
    "    # new data\n",
    "    new_ids = []\n",
    "    for code in df['code']:\n",
    "        new_ids.append(country_to_id_mapping[code])\n",
    "    df['country_id'] = new_ids\n",
    "    \n",
    "    df['top_5'] = (df['rank'] < 5)*1\n",
    "    df['top_10'] = (df['rank'] < 10)*1\n",
    "    df['top_15'] = (df['rank'] < 15)*1\n",
    "    df['top_25'] = (df['rank'] < 25)*1\n",
    "    df['top_50'] = (df['rank'] < 50)*1\n",
    "    df['top_100'] = (df['rank'] < 100)*1\n",
    "    \n",
    "    # dropping columns\n",
    "    df = df.drop(columns = ['problem1','problem2','problem3','problem4','problem5','problem6'])\n",
    "    \n",
    "    df['next_year_winner'] = 0\n",
    "    new_dfs = []\n",
    "    for year in df['year'].unique():\n",
    "        if year < 2020:\n",
    "            tmp1 = df[df['year'] == year + 1].copy()\n",
    "            next_year_winner = tmp1[tmp1['rank'] == 1]['country_id'].iloc[0]\n",
    "            new_df = df[df['year'] == year].copy()\n",
    "            new_df['next_year_winner'] = (new_df['country_id'] == next_year_winner)*1\n",
    "            \n",
    "            for i in [5,10,15,25,50,100]:\n",
    "                top_i = tmp1[tmp1['top_'+str(i)] == 1]\n",
    "                countries = list(top_i['country_id'].unique())\n",
    "                new_df['next_year_top_'+str(i)] = new_df['country_id'].isin(countries)*1\n",
    "                \n",
    "            \n",
    "            new_dfs.append(new_df)\n",
    "        else:\n",
    "            new_df = df[df['year'] == 2020].copy()\n",
    "            new_df['next_year_winner'] = 0\n",
    "            new_dfs.append(new_df)\n",
    "            \n",
    "    df = pd.concat(new_dfs)\n",
    "    \n",
    "    dfs = []\n",
    "    for code in df['code'].unique():\n",
    "        code_df = df[df['code'] == code].sort_values(by = ['year'])\n",
    "        code_df['total_wins'] = code_df['winner'].cumsum()\n",
    "        code_df['total_gold'] = code_df['gold_medals'].cumsum()\n",
    "        code_df['total_silver'] = code_df['silver_medals'].cumsum()\n",
    "        code_df['total_bronze'] = code_df['bronze_medals'].cumsum()\n",
    "        code_df['min_rank'] = code_df['rank'].cummin()\n",
    "        code_df['max_rank'] = code_df['rank'].cummax()\n",
    "        code_df['average_rank'] = code_df['rank'].cumsum()/np.arange(1,code_df.shape[0]+1)\n",
    "        for i in [2,3,4]:\n",
    "            code_df['average_rank_'+str(i)] = code_df['rank'].rolling(window = i).mean()\n",
    "            code_df['recent_wins_'+str(i)] = code_df['rank'].rolling(window = i).sum()\n",
    "\n",
    "        dfs.append(code_df)\n",
    "    df = pd.concat(dfs).fillna(0)\n",
    "\n",
    "    return df, country_to_id_mapping,id_to_country_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df, country_to_id_mapping,id_to_country_mapping = process_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['total', 'rank','winner', 'country_id', \n",
    "            'top_5','top_10', 'top_15', 'top_25', 'top_50', 'top_100',\n",
    "            'total_wins', 'min_rank',\n",
    "            'max_rank', 'average_rank', 'average_rank_2', 'recent_wins_2',\n",
    "            'average_rank_3', 'recent_wins_3', 'average_rank_4', 'recent_wins_4']\n",
    "\n",
    "target = 'next_year_winner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df[df['year'] <= 2019],  df[df['year'] == 2020]\n",
    "X_train, y_train = train[features], train[target].astype(int)\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "95/95 [==============================] - 0s 976us/step - loss: 3.5717 - accuracy: 0.8620\n",
      "Epoch 2/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.9777\n",
      "Epoch 3/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9904\n",
      "Epoch 4/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9883\n",
      "Epoch 5/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0354 - accuracy: 0.9904\n",
      "Epoch 6/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 0.9894\n",
      "Epoch 7/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9862\n",
      "Epoch 8/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.9883\n",
      "Epoch 9/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0310 - accuracy: 0.9862\n",
      "Epoch 10/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9883\n",
      "Epoch 11/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.9883\n",
      "Epoch 12/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9862\n",
      "Epoch 13/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9894\n",
      "Epoch 14/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 0.9926\n",
      "Epoch 15/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0258 - accuracy: 0.9904\n",
      "Epoch 16/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9894\n",
      "Epoch 17/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0274 - accuracy: 0.9894\n",
      "Epoch 18/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0298 - accuracy: 0.9883\n",
      "Epoch 19/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9873\n",
      "Epoch 20/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.9894\n",
      "Epoch 21/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.9894\n",
      "Epoch 22/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0259 - accuracy: 0.9894\n",
      "Epoch 23/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.9873\n",
      "Epoch 24/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.9926\n",
      "Epoch 25/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 0.9915\n",
      "Epoch 26/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9873\n",
      "Epoch 27/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9894\n",
      "Epoch 28/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9904\n",
      "Epoch 29/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 0.9894\n",
      "Epoch 30/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0295 - accuracy: 0.9873\n",
      "Epoch 31/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9894\n",
      "Epoch 32/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0296 - accuracy: 0.9873\n",
      "Epoch 33/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.9883\n",
      "Epoch 34/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.9883\n",
      "Epoch 35/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9904\n",
      "Epoch 36/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0295 - accuracy: 0.9873\n",
      "Epoch 37/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 0.9915\n",
      "Epoch 38/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9904\n",
      "Epoch 39/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0241 - accuracy: 0.9894\n",
      "Epoch 40/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0364 - accuracy: 0.9873\n",
      "Epoch 41/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.9904\n",
      "Epoch 42/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9904\n",
      "Epoch 43/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.9894\n",
      "Epoch 44/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0263 - accuracy: 0.9883\n",
      "Epoch 45/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9958\n",
      "Epoch 46/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.9841\n",
      "Epoch 47/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0283 - accuracy: 0.9862\n",
      "Epoch 48/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0269 - accuracy: 0.9915\n",
      "Epoch 49/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0234 - accuracy: 0.9883\n",
      "Epoch 50/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0258 - accuracy: 0.9915\n",
      "Epoch 51/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0284 - accuracy: 0.9926\n",
      "Epoch 52/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9915\n",
      "Epoch 53/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9904\n",
      "Epoch 54/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9873 ETA: 0s - loss: 0.0271 - accuracy: 0.\n",
      "Epoch 55/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9904\n",
      "Epoch 56/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9873\n",
      "Epoch 57/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 0.9915\n",
      "Epoch 58/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.9904\n",
      "Epoch 59/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.9915\n",
      "Epoch 60/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0235 - accuracy: 0.9915\n",
      "Epoch 61/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0232 - accuracy: 0.9894\n",
      "Epoch 62/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0274 - accuracy: 0.9862\n",
      "Epoch 63/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0266 - accuracy: 0.9862\n",
      "Epoch 64/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0268 - accuracy: 0.9862\n",
      "Epoch 65/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0313 - accuracy: 0.9851\n",
      "Epoch 66/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9904\n",
      "Epoch 67/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9904\n",
      "Epoch 68/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9883\n",
      "Epoch 69/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9926\n",
      "Epoch 70/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9926\n",
      "Epoch 71/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 0.9883\n",
      "Epoch 72/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9915\n",
      "Epoch 73/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0235 - accuracy: 0.9904\n",
      "Epoch 74/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9904\n",
      "Epoch 75/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9936\n",
      "Epoch 76/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.9883\n",
      "Epoch 77/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9926\n",
      "Epoch 78/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9947\n",
      "Epoch 79/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9873\n",
      "Epoch 80/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 0.9873\n",
      "Epoch 81/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0253 - accuracy: 0.9883\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9915\n",
      "Epoch 83/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0346 - accuracy: 0.9883\n",
      "Epoch 84/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9873\n",
      "Epoch 85/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9873\n",
      "Epoch 86/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0232 - accuracy: 0.9904\n",
      "Epoch 87/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0265 - accuracy: 0.9894\n",
      "Epoch 88/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9883\n",
      "Epoch 89/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9894\n",
      "Epoch 90/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.9904\n",
      "Epoch 91/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9894\n",
      "Epoch 92/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0217 - accuracy: 0.9904\n",
      "Epoch 93/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9873\n",
      "Epoch 94/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.9904\n",
      "Epoch 95/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9873\n",
      "Epoch 96/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9894\n",
      "Epoch 97/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9904\n",
      "Epoch 98/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9894\n",
      "Epoch 99/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9915\n",
      "Epoch 100/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9904\n",
      "Epoch 101/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.9926\n",
      "Epoch 102/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 0.9926\n",
      "Epoch 103/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9915\n",
      "Epoch 104/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9894\n",
      "Epoch 105/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 0.9904\n",
      "Epoch 106/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.9926\n",
      "Epoch 107/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9862\n",
      "Epoch 108/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9904\n",
      "Epoch 109/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9904\n",
      "Epoch 110/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.9894\n",
      "Epoch 111/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0281 - accuracy: 0.9894\n",
      "Epoch 112/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 0.9883\n",
      "Epoch 113/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.9904\n",
      "Epoch 114/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.9915\n",
      "Epoch 115/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.9894\n",
      "Epoch 116/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9915\n",
      "Epoch 117/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0257 - accuracy: 0.9915\n",
      "Epoch 118/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.9883\n",
      "Epoch 119/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9894\n",
      "Epoch 120/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0235 - accuracy: 0.9883\n",
      "Epoch 121/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 0.9883\n",
      "Epoch 122/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9915\n",
      "Epoch 123/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9926\n",
      "Epoch 124/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9894\n",
      "Epoch 125/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.9894\n",
      "Epoch 126/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0228 - accuracy: 0.9926\n",
      "Epoch 127/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9894\n",
      "Epoch 128/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9894\n",
      "Epoch 129/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.9915\n",
      "Epoch 130/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0216 - accuracy: 0.9926\n",
      "Epoch 131/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9904\n",
      "Epoch 132/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9915\n",
      "Epoch 133/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0217 - accuracy: 0.9915\n",
      "Epoch 134/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9883\n",
      "Epoch 135/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9904\n",
      "Epoch 136/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0234 - accuracy: 0.9894\n",
      "Epoch 137/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9883\n",
      "Epoch 138/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9936\n",
      "Epoch 139/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0232 - accuracy: 0.9915\n",
      "Epoch 140/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9915\n",
      "Epoch 141/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0235 - accuracy: 0.9904\n",
      "Epoch 142/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0203 - accuracy: 0.9915\n",
      "Epoch 143/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.9894\n",
      "Epoch 144/150\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9915\n",
      "Epoch 145/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0241 - accuracy: 0.9894\n",
      "Epoch 146/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9915\n",
      "Epoch 147/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9936\n",
      "Epoch 148/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0232 - accuracy: 0.9904\n",
      "Epoch 149/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0228 - accuracy: 0.9915\n",
      "Epoch 150/150\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ffa3e495c8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>prob</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA</td>\n",
       "      <td>[0.35674542]</td>\n",
       "      <td>[0.55459446]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHN</td>\n",
       "      <td>[0.2452125]</td>\n",
       "      <td>[0.3248762]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RUS</td>\n",
       "      <td>[0.14422467]</td>\n",
       "      <td>[0.168531]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KOR</td>\n",
       "      <td>[0.07283309]</td>\n",
       "      <td>[0.07855445]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THA</td>\n",
       "      <td>[0.010528684]</td>\n",
       "      <td>[0.010640716]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGP</td>\n",
       "      <td>[3.4686323e-07]</td>\n",
       "      <td>[3.4686335e-07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UKR</td>\n",
       "      <td>[3.2689397e-07]</td>\n",
       "      <td>[3.2689405e-07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>POL</td>\n",
       "      <td>[1.3064596e-07]</td>\n",
       "      <td>[1.3064597e-07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JPN</td>\n",
       "      <td>[1.567338e-08]</td>\n",
       "      <td>[1.567338e-08]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UNK</td>\n",
       "      <td>[9.543903e-09]</td>\n",
       "      <td>[9.543903e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>VNM</td>\n",
       "      <td>[9.298046e-09]</td>\n",
       "      <td>[9.298046e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AUS</td>\n",
       "      <td>[6.822099e-09]</td>\n",
       "      <td>[6.822099e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>LUX</td>\n",
       "      <td>[6.539497e-09]</td>\n",
       "      <td>[6.539497e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TWN</td>\n",
       "      <td>[5.1991043e-09]</td>\n",
       "      <td>[5.1991043e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HUN</td>\n",
       "      <td>[4.953427e-09]</td>\n",
       "      <td>[4.953427e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>CRI</td>\n",
       "      <td>[4.132252e-09]</td>\n",
       "      <td>[4.132252e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CAN</td>\n",
       "      <td>[3.0775484e-09]</td>\n",
       "      <td>[3.0775484e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ISL</td>\n",
       "      <td>[2.9846718e-09]</td>\n",
       "      <td>[2.9846718e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>PRI</td>\n",
       "      <td>[2.8031981e-09]</td>\n",
       "      <td>[2.8031981e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ROU</td>\n",
       "      <td>[2.7190574e-09]</td>\n",
       "      <td>[2.7190574e-09]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country             prob             odds\n",
       "1      USA     [0.35674542]     [0.55459446]\n",
       "0      CHN      [0.2452125]      [0.3248762]\n",
       "3      RUS     [0.14422467]       [0.168531]\n",
       "11     KOR     [0.07283309]     [0.07855445]\n",
       "4      THA    [0.010528684]    [0.010640716]\n",
       "2      SGP  [3.4686323e-07]  [3.4686335e-07]\n",
       "14     UKR  [3.2689397e-07]  [3.2689405e-07]\n",
       "13     POL  [1.3064596e-07]  [1.3064597e-07]\n",
       "10     JPN   [1.567338e-08]   [1.567338e-08]\n",
       "16     UNK   [9.543903e-09]   [9.543903e-09]\n",
       "30     VNM   [9.298046e-09]   [9.298046e-09]\n",
       "22     AUS   [6.822099e-09]   [6.822099e-09]\n",
       "73     LUX   [6.539497e-09]   [6.539497e-09]\n",
       "7      TWN  [5.1991043e-09]  [5.1991043e-09]\n",
       "23     HUN   [4.953427e-09]   [4.953427e-09]\n",
       "66     CRI   [4.132252e-09]   [4.132252e-09]\n",
       "15     CAN  [3.0775484e-09]  [3.0775484e-09]\n",
       "72     ISL  [2.9846718e-09]  [2.9846718e-09]\n",
       "80     PRI  [2.8031981e-09]  [2.8031981e-09]\n",
       "6      ROU  [2.7190574e-09]  [2.7190574e-09]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs = model.predict(X_test)\n",
    "country_ids = X_test['country_id']\n",
    "for_df = []\n",
    "for i,j in zip(pred_probs, country_ids):\n",
    "    win_prob = i\n",
    "    country = id_to_country_mapping[j]\n",
    "    odds = win_prob/(1-win_prob)\n",
    "    for_df.append({'country':country,'prob':win_prob,'odds':odds})\n",
    "results = pd.DataFrame(for_df)\n",
    "results.to_csv('winning_odds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'activation':[\"logistic\", \"relu\"],\n",
    "             'solver' : ['lbfgs','sgd','adam'],\n",
    "             'alpha' : [0.00001,0.0001,0.001,0.01]}\n",
    "nn = MLPClassifier(random_state = 42, max_iter = 1000)\n",
    "clf = GridSearchCV(nn, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "optimal_params = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic', 'alpha': 1e-05, 'solver': 'sgd'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>prob</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHN</td>\n",
       "      <td>3.225284e-01</td>\n",
       "      <td>4.760767e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA</td>\n",
       "      <td>3.129136e-01</td>\n",
       "      <td>4.554211e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RUS</td>\n",
       "      <td>2.055890e-01</td>\n",
       "      <td>2.587942e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KOR</td>\n",
       "      <td>4.468823e-02</td>\n",
       "      <td>4.677868e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THA</td>\n",
       "      <td>2.448860e-02</td>\n",
       "      <td>2.510334e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UKR</td>\n",
       "      <td>8.981567e-05</td>\n",
       "      <td>8.982374e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGP</td>\n",
       "      <td>6.418087e-05</td>\n",
       "      <td>6.418498e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>POL</td>\n",
       "      <td>5.638332e-05</td>\n",
       "      <td>5.638650e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UNK</td>\n",
       "      <td>2.993448e-06</td>\n",
       "      <td>2.993457e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AUS</td>\n",
       "      <td>3.984886e-07</td>\n",
       "      <td>3.984887e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HUN</td>\n",
       "      <td>3.795249e-07</td>\n",
       "      <td>3.795250e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JPN</td>\n",
       "      <td>3.575002e-07</td>\n",
       "      <td>3.575004e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>VNM</td>\n",
       "      <td>1.345879e-07</td>\n",
       "      <td>1.345879e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ITA</td>\n",
       "      <td>6.179137e-08</td>\n",
       "      <td>6.179137e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CAN</td>\n",
       "      <td>8.726039e-09</td>\n",
       "      <td>8.726039e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ROU</td>\n",
       "      <td>4.693165e-09</td>\n",
       "      <td>4.693165e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TWN</td>\n",
       "      <td>3.131757e-09</td>\n",
       "      <td>3.131757e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IRN</td>\n",
       "      <td>1.992464e-09</td>\n",
       "      <td>1.992464e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SRB</td>\n",
       "      <td>9.689542e-10</td>\n",
       "      <td>9.689542e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BRA</td>\n",
       "      <td>3.270459e-10</td>\n",
       "      <td>3.270459e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country          prob          odds\n",
       "0      CHN  3.225284e-01  4.760767e-01\n",
       "1      USA  3.129136e-01  4.554211e-01\n",
       "3      RUS  2.055890e-01  2.587942e-01\n",
       "11     KOR  4.468823e-02  4.677868e-02\n",
       "4      THA  2.448860e-02  2.510334e-02\n",
       "14     UKR  8.981567e-05  8.982374e-05\n",
       "2      SGP  6.418087e-05  6.418498e-05\n",
       "13     POL  5.638332e-05  5.638650e-05\n",
       "16     UNK  2.993448e-06  2.993457e-06\n",
       "22     AUS  3.984886e-07  3.984887e-07\n",
       "23     HUN  3.795249e-07  3.795250e-07\n",
       "10     JPN  3.575002e-07  3.575004e-07\n",
       "30     VNM  1.345879e-07  1.345879e-07\n",
       "17     ITA  6.179137e-08  6.179137e-08\n",
       "15     CAN  8.726039e-09  8.726039e-09\n",
       "6      ROU  4.693165e-09  4.693165e-09\n",
       "7      TWN  3.131757e-09  3.131757e-09\n",
       "8      IRN  1.992464e-09  1.992464e-09\n",
       "24     SRB  9.689542e-10  9.689542e-10\n",
       "18     BRA  3.270459e-10  3.270459e-10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_train, y_train)\n",
    "pred_probs = model.predict_proba(X_test)\n",
    "country_ids = X_test['country_id']\n",
    "for_df = []\n",
    "for i,j in zip(pred_probs, country_ids):\n",
    "    win_prob = i[1]\n",
    "    country = id_to_country_mapping[j]\n",
    "    odds = win_prob/(1-win_prob)\n",
    "    for_df.append({'country':country,'prob':win_prob,'odds':odds})\n",
    "results = pd.DataFrame(for_df)\n",
    "results.sort_values(by = ['prob'], ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9904458598726115"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>prob</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHN</td>\n",
       "      <td>0.446413</td>\n",
       "      <td>0.806401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA</td>\n",
       "      <td>0.322227</td>\n",
       "      <td>0.475421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KOR</td>\n",
       "      <td>0.139072</td>\n",
       "      <td>0.161538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RUS</td>\n",
       "      <td>0.114708</td>\n",
       "      <td>0.129571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THA</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.009505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGP</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.008252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UKR</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>0.006744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CAN</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.006223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ROU</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.006109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BRA</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.005818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JPN</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.005776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HUN</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UNK</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.005601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TWN</td>\n",
       "      <td>0.005463</td>\n",
       "      <td>0.005493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IRN</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GER</td>\n",
       "      <td>0.004770</td>\n",
       "      <td>0.004793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TUR</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>0.004633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>POL</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.004569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ISR</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.004212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ITA</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.003958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country      prob      odds\n",
       "0      CHN  0.446413  0.806401\n",
       "1      USA  0.322227  0.475421\n",
       "11     KOR  0.139072  0.161538\n",
       "3      RUS  0.114708  0.129571\n",
       "4      THA  0.009416  0.009505\n",
       "2      SGP  0.008185  0.008252\n",
       "14     UKR  0.006699  0.006744\n",
       "15     CAN  0.006185  0.006223\n",
       "6      ROU  0.006072  0.006109\n",
       "18     BRA  0.005784  0.005818\n",
       "10     JPN  0.005743  0.005776\n",
       "23     HUN  0.005669  0.005701\n",
       "16     UNK  0.005570  0.005601\n",
       "7      TWN  0.005463  0.005493\n",
       "8      IRN  0.005428  0.005458\n",
       "9      GER  0.004770  0.004793\n",
       "5      TUR  0.004612  0.004633\n",
       "13     POL  0.004548  0.004569\n",
       "21     ISR  0.004195  0.004212\n",
       "17     ITA  0.003943  0.003958"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pipe = Pipeline([('pca',PCA()),('minmax',MinMaxScaler()),('nn', MLPClassifier())])\n",
    "nn_pipe.fit(X_train, y_train)\n",
    "pred_probs = nn_pipe.predict_proba(X_test)\n",
    "country_ids = X_test['country_id']\n",
    "for_df = []\n",
    "for i,j in zip(pred_probs, country_ids):\n",
    "    win_prob = i[1]\n",
    "    country = id_to_country_mapping[j]\n",
    "    odds = win_prob/(1-win_prob)\n",
    "    for_df.append({'country':country,'prob':win_prob,'odds':odds})\n",
    "results = pd.DataFrame(for_df)\n",
    "results.sort_values(by = ['prob'], ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9904458598726115"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [5,10,15,25,50,100]:\n",
    "    target = 'next_year_top_'+str(i)\n",
    "    train, test = df[df['year'] <= 2019],  df[df['year'] == 2020]\n",
    "    X_train, y_train = train[features], train[target].astype(int)\n",
    "    X_test = test[features]\n",
    "\n",
    "    model = MLPClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    model.score(X_train, y_train)\n",
    "    \n",
    "    pred_probs = model.predict_proba(X_test)\n",
    "    country_ids = X_test['country_id']\n",
    "    for_df = []\n",
    "    for i,j in zip(pred_probs, country_ids):\n",
    "        win_prob = i[1]\n",
    "        country = id_to_country_mapping[j]\n",
    "        odds = win_prob/(1-win_prob)\n",
    "        for_df.append({'country':country,'prob':win_prob,'odds':odds})\n",
    "    results = pd.DataFrame(for_df)\n",
    "    results.to_csv('odds_top_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
